# Graph



import networkx as nx

import matplotlib.pyplot as plt

import numpy as np



# Breadth First Search



def bfs(visited, graph, node):

    visited.append(node)

    queue.append(node)



    while queue:

        s = queue.pop(0) 

        print (s, end = " ") 



        for neighbour in graph[s]:

            if neighbour not in visited:

                visited.append(neighbour)

                queue.append(neighbour)



graph = {

        'A': ['B', 'C', 'D'], 

        'B': ['A', 'D', 'E'], 

        'C': ['A', 'D'], 

        'D': ['B', 'C', 'A', 'E'], 

        'E': ['B', 'D']

    }



visited = [] # List to keep track of visited nodes.

queue = []



bfs(visited, graph, 'A')



# dfs



def dfs(graph, source, visited, dfs_traversal):    

    if source not in visited:

        dfs_traversal.append(source)

        #visited.add(source)

        visited.append(source)

        print (source)



        for neighbor_node in graph[source]:

            dfs(graph, neighbor_node, visited, dfs_traversal)

        

    return dfs_traversal



visited = list()

dfs_traversal = list()



graph = {

  'A': ['B', 'C', 'D'], 

  'B': ['A', 'D', 'E'], 

  'C': ['A', 'D'], 

  'D': ['B', 'C', 'A', 'E'], 

  'E': ['B', 'D']

}



dfs(graph, 'A', visited, dfs_traversal)









# To create a graph given the edges and nodes



edges = [['A','B'], ['A','C'], ['A','D'],['D','E'], ['B','E']]

nodes = ['A','B','C','D','E']



graph = {}

for key in nodes:

    graph[key] = []



for (u,v) in edges:

    #print (u, v)

    graph[u].append(v)

    graph[v].append(u)



print (graph)



G = nx.Graph(graph)

nx.draw(G, with_labels=True)



# To find if graph contains a cycle of not



nodes = [0,1,2,3,4]

edges = [[0,1], [0,2], [2,3], [3,4], [1,2]]



graph = {}

for key in nodes:

    graph[key] = []



for (u,v) in edges:

    graph[u].append(v)

    graph[v].append(u)



graph



G = nx.Graph(graph)

nx.draw(G, with_labels=True)



def iscycle(graph, node, visited, par):

    visited[node] = True

    for n in graph[node]:

        if visited[n] == False:

            if iscycle(graph, n, visited, node):

                return True

        elif par != n:

            return True

    return False



visited = [False]*len(nodes)

print (iscycle(graph, 0, visited, par = -1))



# 4 



# Simulate 'Toss of coin' experiment using binomial distribution.



# Prove law of large number, i.e, sample mean $\bar{x}$ approches 

# to population mean $\mu$, if number of trials approches to infinity. 

# Show the plot having number of trials on x-axis and sample mean value of y-axis.



import numpy as np

import matplotlib.pyplot as plt



n = 100

p = 0.5

N = 1000



H = np.random.binomial(n, p, N)



plt.hist(H, bins=20)

plt.xlabel('outcome (Heads)')

plt.ylabel('Frequency')

plt.title('Histogram of random varaible H')

plt.show()



# To get the probability distribution

P = H / n                     



plt.hist(P, bins=20)

plt.xlabel('Probability of occurance of H')

plt.ylabel('Frequency')

plt.title('Probability distribution')

plt.show()



# To calculate the mean using probability distribution

mean = 0

for i in range(len(P)):

    mean = mean +  H[i] * P[i] / np.sum(P) #np.sum(P) makes the distribution normalised



print (mean)



# To compare the mean to that of numpy function

print (np.mean(H))



# To calculate the variance using probability distribution

var = 0

for i in range(len(P)):

    var = var + (H[i] - mean)**2 * P[i]/np.sum(P)



print (var)



# To compare the variance to that of numpy function

print (np.var(H))



# To prove V(x) > V'(x) for |x - mean| >= epsilon



eps = 3.0

var1 = 0

for i in range(len(P)):

    if abs(H[i] - mean) >= eps:

        var1 = var1 + (H[i] - mean)**2 * P[i]/np.sum(P)



print (var1)



# Law of large number



N = np.arange(10, 10000, 10)

population_mean = 0.5

sample_mean = []

n_coins = 100

for i in N:

    H = np.random.binomial(n_coins, population_mean, i)

    sample_mean.append(np.sum(H) / i)



plt.plot(N, sample_mean)

plt.xlabel('Number of trials')

plt.ylabel('Sample mean')

plt.show()



# 10



# Distance between two points  x  and  y  in  d - dimensions is given by:

x1, y1 = 5, 8
x2, y2 = 1, 9

distance = np.sqrt((x1 - y1)**2 + (x2 - y2)**2)
print (distance)

plt.plot (x1, y1, "*", color='b')
plt.plot(x2,y2, "*", color='r')
plt.xlim(0,10)
plt.ylim(0,10)
plt.grid()
plt.show()

x1, y1, z1 = 5, 8, 4
x2, y2, z2 = 1, 9, 7

distance = np.sqrt((x1 - y1)**2 + (x2 - y2)**2 + (z1 - z2)**2)
print (distance)

fig = plt.figure(figsize=(6,6))
ax = plt.axes(projection ='3d')
ax.plot([x1,x2], [y1, y2], [z1,z2], '*' ,color='b')
#ax.plot(x2, y2, z2, '*', color='r')
ax.set_xlim(0,10)
ax.set_ylim(0,10)
ax.set_zlim(0,10)
ax.set_xlabel('x', fontsize=14)
ax.set_ylabel('y', fontsize=14)
ax.set_zlabel('z', fontsize=14)
plt.show()

# distance for dimension d=2

N = 600
A = np.random.uniform(0,10, N)
B = np.random.uniform(0,10, N)


dist2 = []
for i in range(N):
    for j in range(N):
        dist2.append(np.sqrt((A[i] - A[j])**2 + (B[i] - B[j])**2 ))   

plt.figure(figsize=(8,6))
plt.title('Distribution of distance for d = 2', fontsize=15)
plt.hist(dist2, histtype='step')
plt.xlabel('distance', fontsize=14)
plt.show()

N = 600
A = np.random.uniform(0,10, N)
B = np.random.uniform(0,10, N)
C = np.random.uniform(0,10, N)

dist3 = []
for i in range(N):
    for j in range(N):
        dist3.append(np.sqrt((A[i] - A[j])**2 + (B[i] - B[j])**2 +  (C[i] - C[j])**2))

plt.figure(figsize=(8,6))
plt.title('Distribution of distance for d = 3', fontsize=15)
plt.hist(dist3, histtype='step')
plt.xlabel('distance', fontsize=14)
plt.show()

# distance for dimension d=4

N = 600
A = np.random.uniform(0,10, N)
B = np.random.uniform(0,10, N)
C = np.random.uniform(0,10, N)
D = np.random.uniform(0,10, N)

dist4 = []
for i in range(N):
    for j in range(N):
        dist4.append(np.sqrt((A[i] - A[j])**2 + (B[i] - B[j])**2 +  (C[i] - C[j])**2 + (D[i] - D[j])**2))


lt.figure(figsize=(8,6))
plt.title('Distribution of distance for d = 4', fontsize=15)
plt.hist(dist4, histtype='step')
plt.xlabel('distance', fontsize=14)
plt.show()

# Using Monte-Carlo method, calculate the volume of n-dimensional unit sphere (R=1)

d = 2 # The number of dimensions
n = int(1e5) # The number of Monte Carlo trials
Zn = 0
for i in range(n):
    x1 = np.random.uniform()
    x2 = np.random.uniform()
    R = np.sqrt(x1**2 + x2**2)     # Calculate distance from the orgin
    if R <= 1:                     
        Zn += 1                    
V2D = 2**d*Zn/n                    # The Monte Carlo calculation of the sphere volume
V2Dexact = np.pi;                 
ratio = V2D/V2Dexact                

print('The volume of a', d, 'D sphere from the Monte Carlo simulation is', V2D)
print('The exact volume is', V2Dexact, 'and the ratio of the two is', V2D/V2Dexact)

d = 3 # The number of dimensions
n = int(1e5) # The number of Monte Carlo trials
Zn = 0
for i in range(n):
    x1 = np.random.uniform()
    x2 = np.random.uniform()
    x3 = np.random.uniform()
    R = np.sqrt(x1**2 + x2**2 + x3**2)         # Calculate distance from the orgin
    if R <= 1:                                
        Zn += 1                                
V3D = 2**d*Zn/n                                # The Monte Carlo calculation of the sphere volume
V3Dexact = (4/3)*np.pi                         
ratio = V3D/V3Dexact                           
print('The volume of a', d, 'D sphere from the Monte Carlo simulation is', V3D)
print('The exact volume is', V3Dexact, 'and the ratio of the two is', V3D/V3Dexact)

d = 5 # The number of dimensions
n = int(1e5) # The number of Monte Carlo trials
Zn = 0
for i in range(n):
    x1 = np.random.uniform()
    x2 = np.random.uniform()
    x3 = np.random.uniform()
    x4 = np.random.uniform()
    x5 = np.random.uniform()
    R = np.sqrt(x1**2 + x2**2 + x3**2 + x4**2 + x5**2)           # Calculate distance from the orgin
    if R <= 1:                                                
        Zn += 1                                                 
V5D = 2**d*Zn/n                                                 # The Monte Carlo calculation of the sphere volume
V5Dexact = (8/15)*np.pi**2                                          
ratio = V5D/V5Dexact                                              # Compare the calculated volume to the known volume
print('The volume of a', d, 'D sphere from the Monte Carlo simulation is', V5D)
print('The exact volume is', V5Dexact, 'and the ratio of the two is', V5D/V5Dexact)

d = 9 # The number of dimensions
n = int(1e6) # The number of Monte Carlo trials
Zn = 0
for i in range(n):
    x = np.random.uniform(0, 1, d)
    Rsq = 0
    for j in range(d):
       Rsq = Rsq + x[j]**2
    R = np.sqrt(Rsq)                              # Calculate distance from the orgin
    if R <= 1:                                    
        Zn += 1                                       
V9D = 2**d*Zn/n                                          # The Monte Carlo calculation of the sphere volume
V9Dexact = (32/945)*np.pi**4                             
ratio = V9D/V9Dexact                            
print('The volume of a', d, 'D sphere from the Monte Carlo simulation is', V9D)
print('The exact volume is', V9Dexact, 'and the ratio of the two is', V9D/V9Dexact)

# Using Gaussian distribution with mean $\mu=0$ and standard deviation $\sigma=1$, 
# generate the random points on $1$-sphere (circle) and $2$-sphere (surface of sphere).

#To generate random points from normal distribution on a circle
N = 1000
mu = 0
sigma = 1
u = np.random.normal(mu, sigma, N)
v = np.random.normal(mu, sigma, N)

d = np.sqrt( u**2 + v**2)

x = u/d
y = v/d

plt.figure(figsize=(6,6))
plt.plot (x, y, '.')
plt.show()

#To generate random points from normal distribution on surface of sphere

N = 1000
mu = 0
sigma = 1
u = np.random.normal(mu, sigma, N)
v = np.random.normal(mu, sigma, N)
w =  np.random.normal(mu, sigma, N)

d = np.sqrt( u**2 + v**2 + w**2)

x = u/d
y = v/d
z = w/d

fig = plt.figure(figsize=(6,6))
ax = plt.axes(projection ='3d')
ax.plot(x, y, z, '.')
ax.set_xlabel('x', fontsize=14)
ax.set_ylabel('y', fontsize=14)
ax.set_zlabel('z', fontsize=14)
plt.show()


# 16



import numpy as np 

# how to creat matrix in numpy
a = np.array([[8,5],[8,5]])
print(a)

at = a.T
print(at)

evalue,evector = np.linalg.eig(a)
print('eign value:',evalue)
print('eign vector:',evector)

##########################################################
### find the evalue evector & u v and sigma 


a = np.array([[4,8,1],[3,3,-2]])
print(a)


at = a.T
print(at)


u = np.dot(a, at)
print(u)

b = at@a
print(b)

uevalue, uevector =np.linalg.eig(u)
print(" eign value's U :", uevalue)
print(" eign vector's U :", uevector)

bevalue,bevector = np.linalg.eig(b)
print("eign value of b:",bevalue)
print("eign vectors of b:",bevector)



## sigma matrix 


bsort = np.sort(bevalue)[::-1]
print(bsort)

sigmab = np.diag(np.sqrt(bsort))
print("sigma matrix:",sigmab)

z = np.array([[0,0,0,],[0,0,0]])
z[:2,:2] = sigmab[:2,:2]
print(z)

bvector = b[np.argsort(bevalue)[::-1]]
print(bvector)

u,s,vt = np.linalg.svd(b)
print(" value  of u :",u)
print("value of s:",s)
print("value of vt:",vt)

import matplotlib.pyplot as plt
from matplotlib.pyplot import*
from matplotlib.image import imread

at = plt.imread(r"C:\Users\Jay\OneDrive\Desktop\2801\lab\albert_e.png") 
at = at[:,:,0]

plt.imshow(at,cmap='gray')
plt.show()
u2,s2,vt2=np.linalg.svd(at)
diag1=np.diag(np.sqrt(s2))
rank = np.linalg.matrix_rank(diag1)

i,j=vt2.shape
diag2=np.zeros((rank,i))

diag2[:rank , :rank]= diag1
print(u2.shape,diag2.shape,vt2.shape)

m =u2@diag2@vt2

plt.imshow(m,cmap='gray')
plt.show()




# 17

Show that for high dimensional Gaussian samples($X≈N(0,I)$) the Euclidean norm($||X||$)
of these samples shifts away from origin as dimensions are increaesed. This implies 
that in high dimensions the samples will be concentrated on the surface of the hyper 
sphere with radius $r≈\sqrt{d}$ rather than around the center of sphere 
(Recall Gaussian Annulus theorem).


**Note:** Use `np.random.randn(N, d)` to generate high dimensional Gaussian samples.
Norm: $$ ||X|| = \sqrt{\Sigma_{i=1}^{d}X_{i}^2}$$

import numpy as np
import matplotlib.pyplot as plt
# Define the norm
def norm(x):
  return np.sqrt(np.sum(np.square(x), axis=1))
N = 500     # sample size
d = [3,5,10,30,100]  # dimensions
plt.figure(figsize=(8,6))
for i in d:
  x = np.random.randn(N, i)
  plt.hist(norm(x), bins=20, histtype='step', label='dim={}'.format(i))
  plt.legend()
  plt.xlabel('norm')
plt.title('Histogram of Norm')
plt.show()


##Exercise-2
Generate three datasets($X_1, X_2,  X_3$)from Gaussian distribution in high dimensions:
$$ X_1 ≈ N(\mu_1, I), \mu_1=[0, 0, 0,....]^T,$$
$$ X_2 ≈ N(\mu_2, I), \mu_2=[5, 0, 0,....]^T,$$
$$ X_3 ≈ N(\mu_3, I), \mu_2=[10, 0, 0,....]^T$$
Show that the distributions of Euclidean distances (measured from origin) of above
samples start to overlap with each other as dimensions are increased (show it for 
dimensions d = 3, 10, 50, 100, 200, 500). This implies that the samples are 
distinguishable (because of different mean values) at low dimensoins but become 
inseparable as dimensions are increased. Hence clustering algorithms 
will fail for higher dimensions.

**Note:** Euclidean distance from origin is same as calculating the norm defined in
Exercise-1


d = 500       #dimension
N = 500       #samples
mu1 = np.zeros([1,d])
mu1[:,0] = 0           #making mu1 as column vector [mu1, 0, 0, 0....]^T
mu2 = np.zeros([1,d])
mu2[:,0] = 5            #making mu2 as column vector [mu2, 0, 0, 0....]^T
mu3 = np.zeros([1,d])
mu3[:,0] = 10           #making mu3 as column vector [mu3, 0, 0, 0....]^T

x1 = np.random.randn(N,d) + mu1[0,:]   #Shift center of distribution to mu1=0
x2 = np.random.randn(N,d) + mu2[0,:]   #Shift center of distribution to mu1=5
x3 = np.random.randn(N,d) + mu3[0,:]   #Shift center of distribution to mu1=10

plt.scatter(x1[:,0], x1[:,1], label='x1')
plt.scatter(x2[:,0], x2[:,1], label='x2')
plt.scatter(x3[:,0], x3[:,1], label='x3')
plt.legend()
plt.xlabel('dim-1')
plt.ylabel('dim-2')
plt.title('Datasets projected on first two dimensions only')
plt.show()


normx1 = norm(x1)   #Euclidean distance from origin for x1
normx2 = norm(x2)   #Euclidean distance from origin for x2
normx3 = norm(x3)   #Euclidean distance from origin for x3


plt.hist(normx1, bins=20, histtype='step', label='x1', density=True)
plt.hist(normx2, bins=20, histtype='step', label='x2', density=True)
plt.hist(normx3, bins=20, histtype='step', label='x3', density=True)
plt.legend()
plt.xlabel('distance from origin')
plt.show()


##Exercise-3
* Using Johnson Linderstrauss lemma, show the relationship between the error(tolerance)
parameter $\epsilon$ and reduced minimum dimensions $k$ for given sample size $N$. 
Repeat the same for different sample size (N = 1e1, 1e3, 1e7).

* Use `GaussianRandomProjection` method from `Scikit-learn` library to reduce the 
dimension of Gaussian samples having samples $N=100$, dimensions $d=5000$ for a 
suitable choice of error parameter $\epsilon$.

* Also show that the Euclidean distances of the transformed data(in reduced dimensions)
 are almost same as that of original data, i.e, absolute difference of Euclidean 
distances between two datasets are close to zero. 

from sklearn.random.projection import johnson_lindenstrauss_min_dim
from sklearn.random.projection import GaussianRandomProjection
from sklearn.metrics.pairwise import euclidean_distances

eps = np.arange(0.1, 0.999, 0.01)
colors = ['b', 'g', 'm', 'c']
m = [1e1, 1e3, 1e7, 1e10]

for i in range(4):
    min_dim = johnson_lindenstrauss_min_dim(n_samples=m[i], eps=eps)
    label = 'Total samples = ' + str(m[i])
    plt.plot(eps, min_dim, c=colors[i], label=label)

plt.xlabel('eps')
plt.ylabel('min dimenstions')
#plt.axhline(y=3.5, color='k', linestyle=':')
plt.legend()
plt.show()

#Original datasets. Samples=1000, dimensions=5000
X_rand = np.random.RandomState(6).rand(1000, 5000)

#projection matrix, error parameter
proj_gauss = GaussianRandomProjection(random_state=0, eps=0.3)  

#Transformed data(reduced dimension space)
X_transformed = proj_gauss.fit_transform(X_rand)

# Check reduced dimensions
print (X_transformed.shape)

print (X_rand)
print (X_transformed)

#Euclidean distances for original data
dist_raw = euclidean_distances(X_rand)

#Euclidean distance for transformed data
dist_transformed = euclidean_distances(X_transformed)

#Absolute difference between the Eulcidean distances
diff = abs(dist_raw - dist_transformed)

plt.hist(diff.flatten(), bins=20)
plt.xlabel('Euclidean distance')
plt.title('Histogram of Absolute difference ')
plt.show()





# 24



import networkx as nx

import numpy as np

import matplotlib.pyplot as plt



def factorial(x):

  f = 1

  for i in range(1, x+1):

      f = f*i

  return f



# Binomial distribution to find average links/degree of random network



def binomial(N, x, p):

  NCx = factorial(N) / (factorial(x) * factorial(N-x))

  prob = NCx * p**(x) * (1 - p)**(N - x)

  return prob



N = 10        #Nodes

Lmax = int(N*(N-1)/2)  # Max links

L = np.arange(0, Lmax+1, 1)



p = 0.3

prob_L = []

for i in L:

   prob_L.append(binomial(Lmax, i, p))



plt.plot (L, prob_L, '.')

plt.xlabel('Number of links')

plt.ylabel('Probability')

plt.show()



# Poisson distribution to find average links/degree of sparse random network



def poisson(x, l):

    return l**(x) * np.exp(-l) / factorial(x)



N = 10

p = 0.3

k = np.arange(0, 20, 1)

print (k)



avg_k = p*(N-1)

prob_k = []

for i in k:



  prob_k.append(poisson(i, avg_k))



plt.plot(k, prob_k, '.')

plt.show()

#end
